{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/6Novabeam/Segmentation_3D/blob/main/notebooks_DeepViewAgg/s3dis_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGxeUS97SO5"
      },
      "source": [
        "# S3DIS\n",
        "\n",
        "This notebook lets you instantiate the **[S3DIS](http://buildingparser.stanford.edu/dataset.html)** dataset from scratch and visualize **3D+2D spherical samples**.\n",
        "\n",
        "Note that you will need **at least 300G** available for the S3DIS raw dataset and **at least 64G** for the processed files at **5cm voxel resolution** and **1024x512 image resolution**.\n",
        "\n",
        "The S3DIS dataset is composed of **6 Folds**, each representing a distinct building. Images used here are equirectangular panoramic pictures. \n",
        "\n",
        "The images and point clouds provided by the [2D-3D-semantics](https://github.com/alexsax/2D-3D-Semantics) repository are not all aligned. The `S3DISFusedDataset` class from `torch_points3d.datasets.segmentation.multimodal.s3dis` deals with correcting some room and camera orientations before processing the data.\n",
        "\n",
        "This dataset is not too large and as such can be entirely loaded into a **64G RAM** memory. For training, we sample this dataset with **3D spheres and all images seeing at least one point in the spheres**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import omegaconf\" || pip install -q omegaconf\n",
        "!pip install -q hydra-core\n",
        "!pip install pykeops > install.log\n",
        "!pip install -q plyfile\n",
        "!pip install -q faiss-gpu\n",
        "!pip install -q torchnet\n",
        "!pip install -q wandb\n",
        "!pip install -q pypng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zPV_k0l7hoO",
        "outputId": "6dca8cab-ae2c-44e6-f484-25ae1131c7a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ModuleNotFoundError: No module named 'omegaconf'\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 117 kB 67.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.9 MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 151 kB 38.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 85.5 MB 104 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "05Gk7z3bjfY7"
      },
      "outputs": [],
      "source": [
        "# Uncomment to use autoreload\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from time import time\n",
        "import omegaconf\n",
        "from omegaconf import OmegaConf\n",
        "start = time()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rPRsoc1G7SPB"
      },
      "outputs": [],
      "source": [
        "# Select you GPU\n",
        "I_GPU = 0\n",
        "import torch\n",
        "torch.cuda.set_device(I_GPU)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMDJKqXCMOyp",
        "outputId": "5205a713-d291-443a-d9ec-521c8ca095a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To know version of cuda on google colab\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLedjOHbRSZG",
        "outputId": "588ff187-f171-4de9-ff37-81f75b0744ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "pu244LdWkZgk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-geometric\n",
        "#import torch\n",
        "print(torch.__version__)\n",
        "!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdZIHoxNY3ov",
        "outputId": "462df34e-85f7-4f4b-cabe-76fbe68b3dc2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.14)\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 24.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-points-kernels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBZDIh85n7XN",
        "outputId": "afcbf4c1-041c-4cb2-99ba-b8a374d06f7e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-points-kernels in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy<1.20 in /usr/local/lib/python3.7/dist-packages (from torch-points-kernels) (1.19.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from torch-points-kernels) (0.51.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torch-points-kernels) (1.12.0+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-points-kernels) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torch-points-kernels) (4.1.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->torch-points-kernels) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->torch-points-kernels) (57.4.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-points-kernels) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-points-kernels) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-points-kernels) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_points_kernels\n",
        "from torch_points_kernels.torchpoints import ball_query"
      ],
      "metadata": {
        "id": "H7Cl0JKnbhc3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys    \n",
        "path_to_module = '/content/drive/MyDrive/3Dsemantic'\n",
        "sys.path.append(path_to_module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqbe5OVAB1gQ",
        "outputId": "610a99f6-a923-4193-aacd-f8e93f59ad49"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jboKE96lsrCa",
        "outputId": "57481894-ccc0-4a87-abbd-9f030599171a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 20 kB 36.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 41.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 40 kB 38.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 51 kB 41.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 5.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = os.path.dirname(os.getcwd())\n",
        "ROOT = os.path.join(DIR, \"..\")\n",
        "sys.path.insert(0, ROOT)\n",
        "sys.path.insert(0, DIR)"
      ],
      "metadata": {
        "id": "jVaNg_I5kL1y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MHUGTdL8vVwQ",
        "outputId": "17c9fdcc-38f3-4feb-de74-af1a359f4008"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/..'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "h9jiE25t7SPG"
      },
      "outputs": [],
      "source": [
        "from torch_points3d.utils.config import hydra_read\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_points3d.core.multimodal.data import MMData\n",
        "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
        "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
        "from torch_points3d.datasets.segmentation.multimodal.s3dis import S3DISFusedDataset, OBJECT_LABEL, OBJECT_COLOR\n",
        "COLORS = OBJECT_COLOR.tolist()\n",
        "CLASSES = list(OBJECT_LABEL.keys()) + ['unlabelled']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rimrvNZz7SPK"
      },
      "source": [
        "If `visualize_mm_data` does not throw any error but the visualization does not appear, you may need to change your plotly renderer below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "333ql5-A7SPN"
      },
      "outputs": [],
      "source": [
        "import plotly.io as pio\n",
        "\n",
        "#pio.renderers.default = 'jupyterlab'        # for local notebook\n",
        "pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuDxggHZ7SPO"
      },
      "source": [
        "## Dataset creation\n",
        "\n",
        "The following will instantiate the dataset. If the data is not found at `DATA_ROOT`, the folder structure will be created there and the raw dataset will be downloaded there. \n",
        "\n",
        "**Memory-friendly tip** : if you have already downloaded the dataset once and simply want to instantiate a new dataset with different preprocessing (*e.g* change 3D or 2D resolution, mapping parameterization, etc), I recommend you manually replicate the folder hierarchy of your already-existing dataset and create a symlink to its `raw/` directory to avoid downloading and storing (very) large files twice.\n",
        "\n",
        "You will find the config file ruling the dataset creation at `conf/data/segmentation/multimodal/s3disfused-sparse.yaml`. You may edit this file or create new configs inheriting from this one using Hydra and create the associated dataset by modifying `dataset_config` accordingly in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "UjglfuCv7SPR"
      },
      "outputs": [],
      "source": [
        "# Set your dataset root directory, where the data was/will be downloaded\n",
        "DATA_ROOT = '/path/to/your/dataset/root/directory'\n",
        "train_is_trainval = False                                     # set to True if you want to the Train set to be Train+Val\n",
        "sample_per_epoch = 2000                                       # number of spheres sampled in the Train set. Corrects class imbalance. Set to 0 for regularly-sampled spheres\n",
        "fold = 5                                                      # fold that will be used as Test\n",
        "\n",
        "dataset_config = 'segmentation/multimodal/s3disfused-sparse'   \n",
        "models_config = 'segmentation/multimodal/sparseconv3d'       # model family\n",
        "model_name = 'Res16UNet34-L4-early'                          # specific model\n",
        "\n",
        "overrides = [\n",
        "    'task=segmentation',\n",
        "    f'data={dataset_config}',\n",
        "    f'models={models_config}',\n",
        "    f'model_name={model_name}',\n",
        "    f'data.dataroot={DATA_ROOT}',\n",
        "    f'data.fold={fold}',\n",
        "]\n",
        "\n",
        "cfg = hydra_read(overrides)\n",
        "# print(OmegaConf.to_yaml(cfg))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmJxAlWv70Ut",
        "outputId": "7b8a9993-b611-495a-c7a6-1a6fef8e3316"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'task': 'segmentation.multimodal', 'class_weight_method': '', 'class': 's3dis.S3DISFusedDataset', 'dataroot': '/path/to/your/dataset/root/directory', 'fold': 5, 'first_subsampling': 0.02, 'use_category': False, 'sample_per_epoch': 3000, 'resolution_3d': 0.02, 'resolution_2d': [1024, 512], 'padding_2d': 8, 'min_size_2d': 64, 'exact_splatting_2d': True, 'pre_collate_transform': [{'transform': 'PointCloudFusion'}, {'transform': 'SaveOriginalPosId'}, {'transform': 'GridSampling3D', 'params': {'size': 0.02}}, {'transform': 'SaveOriginalPosId', 'params': {'key': 'mapping_index'}}, {'transform': 'PCAComputePointwise', 'params': {'num_neighbors': 50, 'use_faiss': False}}, {'transform': 'EigenFeatures', 'params': {'norm': True, 'linearity': True, 'planarity': True, 'scattering': True}}, {'transform': 'RemoveAttributes', 'params': {'attr_names': ['eigenvalues', 'eigenvectors']}}], 'train_transforms': [{'transform': 'RandomNoise', 'params': {'sigma': 0.001}}, {'transform': 'RandomRotate', 'params': {'degrees': 180, 'axis': 2}}, {'transform': 'RandomScaleAnisotropic', 'params': {'scales': [0.8, 1.2]}}, {'transform': 'RandomSymmetry', 'params': {'axis': [True, False, False]}}, {'transform': 'XYZFeature', 'params': {'add_x': False, 'add_y': False, 'add_z': True}}, {'transform': 'AddFeatsByKeys', 'params': {'feat_names': ['pos_z', 'rgb', 'linearity', 'norm', 'planarity', 'scattering'], 'list_add_to_x': [True, False, False, False, False, False], 'delete_feats': [True, True, True, True, True, True]}}, {'transform': 'Center'}, {'transform': 'GridSampling3D', 'params': {'size': 0.02, 'quantize_coords': True, 'mode': 'last'}}, {'transform': 'ShiftVoxels'}], 'test_transform': [{'transform': 'XYZFeature', 'params': {'add_x': False, 'add_y': False, 'add_z': True}}, {'transform': 'AddFeatsByKeys', 'params': {'feat_names': ['pos_z', 'rgb', 'linearity', 'norm', 'planarity', 'scattering'], 'list_add_to_x': [True, False, False, False, False, False], 'delete_feats': [True, True, True, True, True, True]}}, {'transform': 'Center'}, {'transform': 'GridSampling3D', 'params': {'size': 0.02, 'quantize_coords': True, 'mode': 'last'}}], 'val_transform': [{'transform': 'XYZFeature', 'params': {'add_x': False, 'add_y': False, 'add_z': True}}, {'transform': 'AddFeatsByKeys', 'params': {'feat_names': ['pos_z', 'rgb', 'linearity', 'norm', 'planarity', 'scattering'], 'list_add_to_x': [True, False, False, False, False, False], 'delete_feats': [True, True, True, True, True, True]}}, {'transform': 'Center'}, {'transform': 'GridSampling3D', 'params': {'size': 0.02, 'quantize_coords': True, 'mode': 'last'}}], 'multimodal': {'modality': 'image', 'settings': {'mapping_key': 'mapping_index', 'proj_upscale': 2, 'r_max': 8, 'r_min': 0.05, 'train_pixel_credit': 4, 'test_pixel_credit': 4, 'k_coverage': 2}, 'pre_transform': [{'transform': 'LoadImages', 'params': {'ref_size': [1024, 512], 'show_progress': True}}, {'transform': 'NonStaticMask', 'params': {'ref_size': [1024, 512], 'proj_upscale': 2, 'n_sample': 5}}, {'transform': 'MapImages', 'params': {'method': 'SplattingVisibility', 'ref_size': [1024, 512], 'proj_upscale': 2, 'use_cuda': True, 'voxel': 0.02, 'r_max': 8, 'r_min': 0.05, 'exact': True, 'verbose': True}}, {'transform': 'NeighborhoodBasedMappingFeatures', 'params': {'k': 50, 'voxel': 0.02, 'density': True, 'occlusion': True, 'use_faiss': False, 'use_cuda': False, 'verbose': True}}], 'train_transforms': [{'transform': 'SelectMappingFromPointId'}, {'transform': 'CenterRoll'}, {'transform': 'PickImagesFromMappingArea', 'params': {'use_bbox': True}}, {'transform': 'CropImageGroups', 'params': {'padding': 8, 'min_size': 64}}, {'transform': 'PickImagesFromMemoryCredit', 'params': {'img_size': [1024, 512], 'n_img': 4, 'k_coverage': 2}}, {'transform': 'JitterMappingFeatures', 'params': {'sigma': 0.02, 'clip': 0.03}}, {'transform': 'ColorJitter', 'params': {'brightness': 0.6, 'contrast': 0.6, 'saturation': 0.7}}, {'transform': 'RandomHorizontalFlip'}, {'transform': 'ToFloatImage'}, {'transform': 'Normalize'}], 'test_transforms': [{'transform': 'SelectMappingFromPointId'}, {'transform': 'CenterRoll'}, {'transform': 'PickImagesFromMappingArea', 'params': {'use_bbox': True}}, {'transform': 'CropImageGroups', 'params': {'padding': 8, 'min_size': 64}}, {'transform': 'PickImagesFromMemoryCredit', 'params': {'img_size': [1024, 512], 'n_img': 4, 'k_coverage': 2}}, {'transform': 'ToFloatImage'}, {'transform': 'Normalize'}], 'val_transforms': [{'transform': 'SelectMappingFromPointId'}, {'transform': 'CenterRoll'}, {'transform': 'PickImagesFromMappingArea', 'params': {'use_bbox': True}}, {'transform': 'CropImageGroups', 'params': {'padding': 8, 'min_size': 64}}, {'transform': 'PickImagesFromMemoryCredit', 'params': {'img_size': [1024, 512], 'n_img': 4, 'k_coverage': 2}}, {'transform': 'ToFloatImage'}, {'transform': 'Normalize'}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylBQom1i7SPT"
      },
      "source": [
        "The dataset will now be created based on the parsed configuration. I recommend having **at least 300G** available for the S3DIS raw dataset and **at least 64G** for the processed files at **5cm voxel resolution** and **1024x512 image resolution**.\n",
        "\n",
        "As long as you do not change core dataset parameters, preprocessing should only be performed once for your dataset. It may take some time, **mostly depending on the 3D and 2D resolutions** you choose to work with (the larger the slower). As a rule of thumb, it took me **between 1 and 2 hours** (excluding download) to preprocess the 2D and 3D data the S3DIS dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "3RP_12dZ7SPU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "e417cadc-e8c0-4a7a-ddac-ce4a63fbc21b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \"\"\"\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-6c3ec184986e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dataset instantiation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS3DISFusedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time = {time() - start:0.1f} sec.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3Dsemantic/torch_points3d/datasets/segmentation/multimodal/s3dis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_opt)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mpre_transform_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_transform_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0mtransform_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_transform_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m             img_ref_size=self.dataset_opt.resolution_2d)\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         self.val_dataset = S3DISSphereMM(\n",
            "\u001b[0;32m/content/drive/MyDrive/3Dsemantic/torch_points3d/datasets/segmentation/multimodal/s3dis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, sample_per_epoch, radius, sample_res, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_radius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grid_sphere_sampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSampling3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3Dsemantic/torch_points3d/datasets/segmentation/multimodal/s3dis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, test_area, split, transform, pre_transform, pre_collate_transform, pre_filter, keep_instance, pre_transform_image, transform_image, img_ref_size, verbose, debug)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         super(S3DISOriginalFusedMM, self).__init__(\n\u001b[0;32m--> 168\u001b[0;31m             root, transform, pre_transform, pre_filter)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     54\u001b[0m                  \u001b[0mpre_transform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                  pre_filter: Optional[Callable] = None):\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'download'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'process'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3Dsemantic/torch_points3d/datasets/segmentation/multimodal/s3dis.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# We have to include this method, otherwise the parent class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# skips download.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3Dsemantic/torch_points3d/datasets/segmentation/multimodal/s3dis.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m                 log.info(\"Press any key to continue, or CTRL-C to exit. By \"\n\u001b[1;32m    281\u001b[0m                          \"continuing, you confirm filling up the form.\")\n\u001b[0;32m--> 282\u001b[0;31m                 \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m                 gdown.download(\n\u001b[1;32m    284\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Dataset instantiation\n",
        "start = time()\n",
        "dataset = S3DISFusedDataset(cfg.data)\n",
        "# print(dataset)\n",
        "print(f\"Time = {time() - start:0.1f} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvwbBhdF7SPV"
      },
      "source": [
        "To visualize the multimodal samples produced by the dataset, we need to remove some of the dataset transforms that affect points, images and mappings. The `sample_real_data` function will be used to get samples without breaking mappings consistency for visualization.\n",
        "\n",
        "At training and evaluation time, these transforms are used for data augmentation, dynamic size batching (see our [paper](https://arxiv.org/submit/4264152)), etc..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import *\n",
        "from torch_points3d.core.data_transform import *\n",
        "from torch_points3d.core.data_transform.multimodal.image import *\n",
        "from torch_points3d.datasets.base_dataset import BaseDataset\n",
        "from torch_points3d.datasets.base_dataset_multimodal import BaseDatasetMM"
      ],
      "metadata": {
        "id": "6isKfhhyuy-f"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kes959qt7SPX"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.transforms import *\n",
        "from torch_points3d.core.data_transform import *\n",
        "from torch_points3d.core.data_transform.multimodal.image import *\n",
        "from torch_points3d.datasets.base_dataset import BaseDataset\n",
        "from torch_points3d.datasets.base_dataset_multimodal import BaseDatasetMM\n",
        "\n",
        "# Transforms on 3D points that we need to exclude for visualization purposes\n",
        "augmentations_3d = [RandomNoise, RandomRotate, RandomScaleAnisotropic, RandomSymmetry, ShiftVoxels]\n",
        "exclude_3d_viz = augmentations_3d + [AddFeatsByKeys, Center, GridSampling3D]\n",
        "\n",
        "# Transforms on 2D images and mappings that we need to exclude for visualization purposes\n",
        "augmentations_2d = [JitterMappingFeatures, ColorJitter, RandomHorizontalFlip]\n",
        "exclude_2d_viz = augmentations_2d + [ToFloatImage, Normalize, PickImagesFromMemoryCredit]\n",
        "\n",
        "\n",
        "def sample_real_data(tg_dataset, idx=0, exclude_3d=None, exclude_2d=None):\n",
        "    \"\"\"\n",
        "    Temporarily remove the 3D and 2D transforms affecting the point \n",
        "    positions and images from the dataset to better visualize points \n",
        "    and images relative positions.\n",
        "    \"\"\"    \n",
        "    # Remove some 3D transforms\n",
        "    transform_3d = tg_dataset.transform\n",
        "    if exclude_3d:\n",
        "        tg_dataset.transform = BaseDataset.remove_transform(transform_3d, exclude_3d)\n",
        "\n",
        "    # Remove some 2D transforms, if any\n",
        "    is_multimodal = hasattr(tg_dataset, 'transform_image')\n",
        "    if is_multimodal and exclude_2d:\n",
        "        transform_2d = tg_dataset.transform_image\n",
        "        tg_dataset.transform_image = BaseDatasetMM.remove_multimodal_transform(transform_2d, exclude_2d)\n",
        "    \n",
        "    # Get a sample from the dataset, with transforms excluded\n",
        "    out = tg_dataset[idx]\n",
        "    \n",
        "    # Restore transforms\n",
        "    tg_dataset.transform = transform_3d\n",
        "    if is_multimodal and exclude_2d:\n",
        "        tg_dataset.transform_image = transform_2d\n",
        "        \n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwJydOET7SPY"
      },
      "source": [
        "## Visualize a single multimodal sample\n",
        "\n",
        "We can now pick samples from the train, val and test datasets.\n",
        "\n",
        "Please refer to `torch_points3d/visualization/multimodal_data` for more details on visualization options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Ssg5QuIi7SPZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "90b26e0a-0d11-45b6-c336-fe9909a40bc8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-c106d68876cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pick a random sphere in the Train set, with balanced class probabilites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmm_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_real_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_3d_viz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_2d_viz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Pick a random sphere in the Val set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# idx = np.random.randint(len(dataset.val_dataset))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample_real_data' is not defined"
          ]
        }
      ],
      "source": [
        "# Pick a random sphere in the Train set, with balanced class probabilites\n",
        "mm_data = sample_real_data(dataset.train_dataset, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
        "\n",
        "# Pick a random sphere in the Val set\n",
        "# idx = np.random.randint(len(dataset.val_dataset))\n",
        "# mm_data = sample_real_data(dataset.val_dataset, idx=idx, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
        "\n",
        "# Pick a random sphere in the Test set\n",
        "# idx = np.random.randint(len(dataset.test_dataset[0]))\n",
        "# mm_data = sample_real_data(dataset.test_dataset[0], idx=idx, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
        "\n",
        "visualize_mm_data(mm_data, class_names=CLASSES, class_colors=COLORS, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, front='y', alpha=0.3, pointsize=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7nWH-S7SPa"
      },
      "source": [
        "## Visualize a whole S3DIS fold\n",
        "\n",
        "We can also view a whole fold.\n",
        "\n",
        "To allow rapid visualization, note the window is subsampled to `voxel=0.25` and the maximum number of points shown is set `max_points=1000000`. Besides `show_2d=False` prevents images from being displayed, which would overload the viewer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2B_DtU87SPb"
      },
      "outputs": [],
      "source": [
        "i_fold = 0  # fold number but does not exactly correspond to the real fold names\n",
        "mm_data_large = MMData(dataset.train_dataset._datas[i_fold], image=ImageData([dataset.train_dataset._images[i_fold]]))\n",
        "# mm_data_large = MMData(dataset.val_dataset._datas[i_fold], dataset.val_dataset._images[i_fold], dataset.val_dataset._mappings[i_fold])\n",
        "# mm_data_large = MMData(dataset.test_dataset[0]._datas[0], dataset.test_dataset[0]._images[0], dataset.test_dataset[0]._mappings[0])\n",
        "visualize_mm_data(mm_data_large, class_names=CLASSES, class_colors=COLORS, figsize=1000, voxel=0.25, show_2d=False, max_points=1000000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhQKbDtk7SPc"
      },
      "source": [
        "## Run inference from pretrained weights and visualize predictions\n",
        "It is possible to visualize the pointwise predictions and errors from a model. \n",
        "\n",
        "To do so, we will use the pretrained weights made available with this project. See `README.md` to get the download links and manually place the `.pt` files locally. You will need to provide `checkpoint_dir` where you saved those files in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBvHg3i27SPd"
      },
      "outputs": [],
      "source": [
        "from torch_points3d.models.model_factory import instantiate_model\n",
        "\n",
        "# Set your parameters\n",
        "checkpoint_dir = '/directory/containing/your/checkpoint/file'\n",
        "\n",
        "# Create the model\n",
        "print(f\"Creating model: {cfg.model_name}\")\n",
        "model = instantiate_model(cfg, dataset)\n",
        "# print(model)\n",
        "\n",
        "# Load the checkpoint and recover the 'latest' model weights\n",
        "checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt')\n",
        "model.load_state_dict_with_same_shape(checkpoint['models']['latest'], strict=False)\n",
        "\n",
        "# Prepare the model for inference\n",
        "model = model.eval().cuda()\n",
        "print('Model loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9YhUZQC7SPe"
      },
      "source": [
        "Now we have loaded the model, we need to run a forward pass on a sample. Howver, if we want to be able to visualize the predictions, we need to pay special attention to which type of 3D and 2D transforms we apply on the data if we do not want to break the mappings. To do so, we will manually apply some sensitive transforms to be able to both infer on the data and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn0VNM5W7SPe"
      },
      "outputs": [],
      "source": [
        "# Pick a random sphere in the Train set, with balanced class probabilites\n",
        "mm_data = sample_real_data(dataset.train_dataset, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
        "\n",
        "# Pick a random sphere in the Val set\n",
        "# idx = np.random.randint(len(dataset.val_dataset))\n",
        "# mm_data = sample_real_data(dataset.val_dataset, idx=idx, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
        "\n",
        "# Pick a random sphere in the Test set\n",
        "# idx = np.random.randint(len(dataset.test_dataset[0]))\n",
        "# mm_data = sample_real_data(dataset.test_dataset[0], idx=idx, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
        "# mm_data = sample_real_data(dataset.test_dataset[0], idx=idx, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
        "\n",
        "# Extract point cloud and images from MMData object\n",
        "data = mm_data.data.clone()\n",
        "images = mm_data.modalities['image'].clone()\n",
        "\n",
        "# For voxel-based 3D backbones such as SparseConv3d and MinkowskiNet, points need to be \n",
        "# preprocessed with Center and GridSampling3D. Unfortunately, Center breaks relative \n",
        "# positions between points and images. Besides, the combination of Center and GridSampling3D\n",
        "# may lead to some points being merged into the same voxels, so we must apply it to both the\n",
        "# inference and visualization data to make sure we have the same voxels. The workaround here \n",
        "# is to manually run these while keeping track of the centering offset\n",
        "center = data.pos.mean(dim=-2, keepdim=True)\n",
        "data = AddFeatsByKeys(list_add_to_x=[True], feat_names=['pos_z'], delete_feats=[True])(data)          # add z-height to the features\n",
        "data = Center()(data)                                                                                 # mean-center the data\n",
        "data = GridSampling3D(data.grid_size, quantize_coords=True, mode='last')(data)                        # quantization for volumetric models\n",
        "\n",
        "# This last voxelization step with GridSampling3D might have removed some points, so we need\n",
        "# to update the mappings usign SelectMappingFromPointId. To control the size of the batch, we\n",
        "# use PickImagesFromMemoryCredit. Besides, 2D models expect normalized float images, which is\n",
        "# why we call ToFloatImage and Normalize\n",
        "data, images = SelectMappingFromPointId()(data, images)                                               # update mappings after GridSampling3D\n",
        "data, images = PickImagesFromMemoryCredit(\n",
        "    img_size=cfg.data.resolution_2d, \n",
        "    k_coverage=cfg.data.multimodal.settings.test_pixel_credit, \n",
        "    n_img=cfg.ata.multimodal.settings.k_coverage)(data, images)                                       # select images to respect memory constraints\n",
        "data, images_infer = ToFloatImage()(data, images.clone())                                             # convert uint8 images to float\n",
        "data, images_infer = Normalize()(data, images_infer)                                                  # RGB normalization\n",
        "\n",
        "# Create a MMData for inference\n",
        "mm_data_infer = MMData(data, image=images_infer)\n",
        "\n",
        "# Create a MMBatch and run inference\n",
        "batch = MMBatch.from_mm_data_list([mm_data_infer])\n",
        "model.set_input(batch, model.device)\n",
        "model(batch)\n",
        "\n",
        "# Create a MMData for visualization\n",
        "data.pos += center\n",
        "mm_data = MMData(data, image=images)\n",
        "\n",
        "# Recover the predicted labels for visualization\n",
        "mm_data.data.pred = model.output.detach().cpu().argmax(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvP1G9Pi7SPf"
      },
      "outputs": [],
      "source": [
        "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=True, front='y', class_names=CLASSES, class_colors=COLORS, alpha=0.3, pointsize=5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "s3dis_visualization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}